{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55561a2",
   "metadata": {},
   "source": [
    "# I. Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3711d5d",
   "metadata": {},
   "source": [
    "## Problem statement :\n",
    "\n",
    "Bank XYZ has been observing a lot of customers closing their accounts or switching to competitor banks over the past couple of quarters. As such, this has caused a huge dent in the quarterly revenues and might drastically affect annual revenues for the ongoing financial year, causing stocks to plunge and market cap to reduce by X %. A team of business, product, engineering and data science folks have been put together to arrest this slide. \n",
    "\n",
    "__Objective__ : Can we build a model to predict, with a reasonable accuracy, the customers who are going to churn in the near future? Being able to accurately estimate when they are going to churn will be an added bonus\n",
    "\n",
    "__Definition of churn__ : A customer having closed all their active accounts with the bank is said to have churned. Churn can be defined in other ways as well, based on the context of the problem. A customer not transacting for 6 months or 1 year can also be defined as to have churned, based on the business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49710af7",
   "metadata": {},
   "source": [
    "__Product Manager's perspective :__  \n",
    "\n",
    "(1) Business goal : Arrest decrease in revenues or loss of active customers of bank\n",
    "\n",
    "(2) Identify data source : There are different source of data. Some of these could be Transactional systems, event-based logs, Data warehouse (MySQL DBs, Redshift/AWS), Data Lakes, NoSQL DBs.\n",
    "\n",
    "(3) Audit for data quality : De-duplication of events/transactions, Complete or partial absence of data for chunks of time in between, Obscuring PII (personal identifiable information) data \n",
    "\n",
    "(4) Business and Data-related metrics : Tracking these metrics over time, probably through some intuitive visualizations\n",
    "    \n",
    "    (i) Business metrics : Churn rate (month-on-month, weekly/quarterly), Trend of avg. number of products per customer, \n",
    "        %age of dormant customers, Other such descriptive metrics\n",
    "    \n",
    "    (ii) Data-related metrics : F1-score, Recall, Precision\n",
    "         Recall = TP/(TP + FN) \n",
    "         Precision = TP/(TP + FP)\n",
    "         F1-score = Harmonic mean of Recall and Precision\n",
    "         where, TP = True Positive, FP = False Positive and FN = False Negative\n",
    "\n",
    "(5) Prediction model output format : These models doesn't require deployment. Instead, we can run these models periodically (monthly/quarterly) and the list of customers along with their propensity to churn can be shared with the business (Sales/Marketing) or Product team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1d0c7",
   "metadata": {},
   "source": [
    "* Business metrics : If we take Recall target as __70%__ which means correctly identifying 70% of customers who's going to churn in the near future, we can expect that due to business intervention (offers, getting in touch with customers etc.), 50% of the customers can be saved from being churned, which means atleast a __35%__ improvement in Churn Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37643fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e33881",
   "metadata": {},
   "source": [
    "# II. Importing Data and Descriptive Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1bf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/BankingCustomerData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d1f8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9684e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5085b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65d00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variables = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Surname_enc', \n",
    "                        'Balance_per_product', 'Balance_by_est_salary', 'Tenure_age_ratio', 'AgeSurname_mean_churn']\n",
    "categorical_variables = ['Gender', 'HasCrCard', 'IsActiveMember', 'Country_France', 'Country_Germany', 'Country_Spain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0f933e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CreditScore',\n",
       " 'Age',\n",
       " 'Tenure',\n",
       " 'Balance',\n",
       " 'NumOfProducts',\n",
       " 'EstimatedSalary',\n",
       " 'Surname_enc',\n",
       " 'Balance_per_product',\n",
       " 'Balance_by_est_salary',\n",
       " 'Tenure_age_ratio',\n",
       " 'AgeSurname_mean_churn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Gender',\n",
       " 'HasCrCard',\n",
       " 'IsActiveMember',\n",
       " 'Country_France',\n",
       " 'Country_Germany',\n",
       " 'Country_Spain']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_variables        \n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611a5fa",
   "metadata": {},
   "source": [
    "### Separating out train-test-valid sets\n",
    "\n",
    "Since this is the only data available to us, we keep aside a holdout/test set to evaluate our model at the very end in order to estimate our chosen model's performance on unseen data / new data.\n",
    "\n",
    "A validation set is also created which we'll use in our baseline models to evaluate and tune our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bee7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating out different columns into various categories as defined above\n",
    "target_variable = ['Exited']\n",
    "cols_to_remove = ['RowNumber', 'CustomerId']\n",
    "\n",
    "# Tenure and NumOfProducts are ordinal variables. \n",
    "continuous_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "# HasCrCard and IsActiveMember are actually binary categorical variables.\n",
    "categorical_features = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59762457",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating out target variable and removing the non-essential columns\n",
    "y = df[target_variable].values\n",
    "df.drop(cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da403bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Keeping aside a test/holdout set\n",
    "df_train_val, df_test, y_train_val, y_test = train_test_split(df, y.ravel(), test_size = 0.2, random_state = 42)\n",
    "\n",
    "## Splitting into train and validation set\n",
    "df_train, df_val, y_train, y_val = train_test_split(df_train_val, y_train_val, test_size = 0.12, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d4cec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7040, 12), (960, 12), (2000, 12), (7040,), (960,), (2000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.20738636363636365, 0.19166666666666668, 0.1965)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape, y_train.shape, y_val.shape, y_test.shape\n",
    "np.mean(y_train), np.mean(y_val), np.mean(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe51f94",
   "metadata": {},
   "source": [
    "### Spot-checking various ML algorithms\n",
    "\n",
    "__Steps__ :\n",
    "\n",
    "- Automate data preparation and model run through Pipelines\n",
    "\n",
    "- Model Zoo : List of all models to compare/spot-check\n",
    "\n",
    "- Evaluate using k-fold Cross validation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78be9e",
   "metadata": {},
   "source": [
    "#### Automating data preparation and model run through Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d6fed4",
   "metadata": {},
   "source": [
    "__Base class for all estimators in scikit-learn.__\n",
    "\n",
    "All estimators should specify all the parameters that can be set at the class level in their \"__ init __ \" as explicit keyword arguments (no *args or **kwargs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d4a4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef3d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    Encodes categorical columns using LabelEncoding, OneHotEncoding and TargetEncoding.\n",
    "    LabelEncoding is used for binary categorical columns\n",
    "    OneHotEncoding is used for columns with <= 10 distinct values\n",
    "    TargetEncoding is used for columns with higher cardinality (>10 distinct values)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols = None, label_encoder_cols = None, onehot_encoder_cols = None, target_encoding_cols = None, \n",
    "                 reduce_df = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : list of str\n",
    "            Columns to encode.  Default is to one-hot/target/label encode all categorical columns in the DataFrame.\n",
    "        reduce_df : bool\n",
    "            Whether to use reduced degrees of freedom for encoding\n",
    "            (that is, add N-1 one-hot columns for a column with N \n",
    "            categories). E.g. for a column with categories A, B, \n",
    "            and C: When reduce_df is True, A=[1, 0], B=[0, 1],\n",
    "            and C=[0, 0].  When reduce_df is False, A=[1, 0, 0], \n",
    "            B=[0, 1, 0], and C=[0, 0, 1]\n",
    "            Default = False\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(cols, str):\n",
    "            self.cols = [cols]\n",
    "        else :\n",
    "            self.cols = cols\n",
    "        \n",
    "        if isinstance(label_encoder_cols, str):\n",
    "            self.label_encoder_cols = [label_encoder_cols]\n",
    "        else :\n",
    "            self.label_encoder_cols = label_encoder_cols\n",
    "        \n",
    "        if isinstance(onehot_encoder_cols, str):\n",
    "            self.onehot_encoder_cols = [onehot_encoder_cols]\n",
    "        else :\n",
    "            self.onehot_encoder_cols = onehot_encoder_cols\n",
    "        \n",
    "        if isinstance(target_encoding_cols, str):\n",
    "            self.target_encoding_cols = [target_encoding_cols]\n",
    "        else :\n",
    "            self.target_encoding_cols = target_encoding_cols\n",
    "        \n",
    "        self.reduce_df = reduce_df\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit label/one-hot/target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode all categorical cols by default\n",
    "        if self.cols is None:\n",
    "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
    "\n",
    "        # Check columns are in X\n",
    "        for col in self.cols:\n",
    "            if col not in X:\n",
    "                raise ValueError('Column \\''+col+'\\' not in X')\n",
    "        \n",
    "        # Separating out lcols, ohecols and tcols\n",
    "        if self.label_encoder_cols is None:\n",
    "            self.label_encoder_cols = [c for c in self.cols if X[c].nunique() <= 2]\n",
    "        \n",
    "        if self.onehot_encoder_cols is None:\n",
    "            self.onehot_encoder_cols = [c for c in self.cols if ((X[c].nunique() > 2) & (X[c].nunique() <= 10))]\n",
    "        \n",
    "        if self.target_encoding_cols is None:\n",
    "            self.target_encoding_cols = [c for c in self.cols if X[c].nunique() > 10]\n",
    "        \n",
    "        \n",
    "        ## Create Label Encoding mapping\n",
    "        self.label_encoder_maps = dict()\n",
    "        for col in self.label_encoder_cols:\n",
    "            self.label_encoder_maps[col] = dict(zip(X[col].values, X[col].astype('category').cat.codes.values))\n",
    "        \n",
    "        \n",
    "        ## Create OneHot Encoding mapping\n",
    "        self.onehot_encoder_maps = dict() #dict to store map for each column\n",
    "        for col in self.onehot_encoder_cols:\n",
    "            self.onehot_encoder_maps[col] = []\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                self.onehot_encoder_maps[col].append(unique)\n",
    "            if self.reduce_df:\n",
    "                del self.onehot_encoder_maps[col][-1]\n",
    "        \n",
    "        \n",
    "        ## Create Target Encoding mapping\n",
    "        self.global_target_mean = y.mean().round(2)\n",
    "        self.sum_count = dict()\n",
    "        for col in self.target_encoding_cols:\n",
    "            self.sum_count[col] = dict()\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                ix = X[col]==unique\n",
    "                self.sum_count[col][unique] = (y[ix].sum(),ix.sum())\n",
    "        \n",
    "        \n",
    "        ## Return the fit object\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform label/one-hot/target encoding transformation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to label encode\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        \n",
    "        Xo = X.copy()\n",
    "        ## Perform label encoding transformation\n",
    "        for col, lmap in self.label_encoder_maps.items():\n",
    "            # Map the column\n",
    "            Xo[col] = Xo[col].map(lmap)\n",
    "            Xo[col].fillna(-1, inplace=True) ## Filling new values with -1\n",
    "        \n",
    "        \n",
    "        ## Perform one-hot encoding transformation\n",
    "        for col, vals in self.onehot_encoder_maps.items():\n",
    "            for val in vals:\n",
    "                new_col = col+'_'+str(val)\n",
    "                Xo[new_col] = (Xo[col]==val).astype('uint8')\n",
    "            del Xo[col]\n",
    "        \n",
    "        \n",
    "        ## Perform LOO target encoding transformation\n",
    "        # Use normal target encoding if this is test data\n",
    "        if y is None:\n",
    "            for col in self.sum_count:\n",
    "                vals = np.full(X.shape[0], np.nan)\n",
    "                for cat, sum_count in self.sum_count[col].items():\n",
    "                    vals[X[col]==cat] = (sum_count[0]/sum_count[1]).round(2)\n",
    "                Xo[col] = vals\n",
    "                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n",
    "\n",
    "        # LOO target encode each column\n",
    "        else:\n",
    "            for col in self.sum_count:\n",
    "                vals = np.full(X.shape[0], np.nan)\n",
    "                for cat, sum_count in self.sum_count[col].items():\n",
    "                    ix = X[col]==cat\n",
    "                    if sum_count[1] > 1:\n",
    "                        vals[ix] = ((sum_count[0]-y[ix].reshape(-1,))/(sum_count[1]-1)).round(2)\n",
    "                    else :\n",
    "                        vals[ix] = ((y.sum() - y[ix])/(X.shape[0] - 1)).round(2) # Catering to the case where a particular \n",
    "                                                                                 # category level occurs only once in the dataset\n",
    "                \n",
    "                Xo[col] = vals\n",
    "                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n",
    "        \n",
    "        \n",
    "        ## Return encoded DataFrame\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via label/one-hot/target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.fit(X, y).transform(X, y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "995db60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Add new, engineered features using original categorical and numerical features of the DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps = 1e-6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eps : A small value to avoid divide by zero error. Default value is 0.000001\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing base columns using which new interaction-based features can be engineered\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "    \n",
    "        # Add 4 new columns - bal_per_product, bal_by_est_salary, tenure_age_ratio, age_surname_mean_churn\n",
    "        Xo['Balance_per_product'] = Xo.Balance/(Xo.NumOfProducts + self.eps)\n",
    "        Xo['Balance_by_est_salary'] = Xo.Balance/(Xo.EstimatedSalary + self.eps)\n",
    "        Xo['Tenure_age_ratio'] = Xo.Tenure/(Xo.Age + self.eps)\n",
    "        Xo['AgeSurname_mean_churn'] = np.sqrt(Xo.Age) * Xo.Surname\n",
    "        \n",
    "        ## Returning the updated dataframe\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing base columns using which new interaction-based features can be engineered\n",
    "        \"\"\"\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e51998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom standard scaler class with the ability to apply scaling on selected columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_cols = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        scaling_cols : list of str\n",
    "            Columns on which to perform scaling and normalization. Default is to scale all numerical columns\n",
    "        \n",
    "        \"\"\"\n",
    "        self.scaling_cols = scaling_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        \n",
    "        # Scaling all non-categorical columns if user doesn't provide the list of columns to scale\n",
    "        if self.scaling_cols is None:\n",
    "            self.scaling_cols = [c for c in X if ((str(X[c].dtype).find('float') != -1) or (str(X[c].dtype).find('int') != -1))]\n",
    "        \n",
    "     \n",
    "        ## Create mapping corresponding to scaling and normalization\n",
    "        self.scaling_maps = dict()\n",
    "        for col in self.scaling_cols:\n",
    "            self.scaling_maps[col] = dict()\n",
    "            self.scaling_maps[col]['mean'] = np.mean(X[col].values).round(2)\n",
    "            self.scaling_maps[col]['std_dev'] = np.std(X[col].values).round(2)\n",
    "        \n",
    "        # Return fit object\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "        \n",
    "        ## Map transformation to respective columns\n",
    "        for col in self.scaling_cols:\n",
    "            Xo[col] = (Xo[col] - self.scaling_maps[col]['mean']) / self.scaling_maps[col]['std_dev']\n",
    "        \n",
    "        \n",
    "        # Return scaled and normalized DataFrame\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        # Fit and return transformed dataframe\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f271cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model, X_training_data, X_validation_data, y_training_data, y_validation_data, flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and validation results\n",
    "    score_list=[] \n",
    "    \n",
    "    pred_train = model.predict(X_training_data)\n",
    "    pred_validation = model.predict(X_validation_data)\n",
    "    \n",
    "    train_acc = accuracy_score(y_training_data, pred_train)\n",
    "    validation_acc = accuracy_score(y_validation_data, pred_validation)\n",
    "    \n",
    "    train_recall = recall_score(y_training_data,pred_train)\n",
    "    validation_recall = recall_score(y_validation_data,pred_validation)\n",
    "    \n",
    "    train_precision = precision_score(y_training_data,pred_train)\n",
    "    validation_precision = precision_score(y_validation_data,pred_validation)\n",
    "    \n",
    "    train_f1 = f1_score(y_training_data,pred_train)\n",
    "    validation_f1 = f1_score(y_validation_data,pred_validation)\n",
    "\n",
    "    score_list.extend((train_acc,validation_acc,train_recall,validation_recall,train_precision,validation_precision,train_f1,validation_f1))\n",
    "    \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",accuracy_score(y_training_data,pred_train))\n",
    "        print(\"Accuracy on validation set : \",accuracy_score(y_validation_data,pred_validation))\n",
    "        print(\"Recall on training set : \",recall_score(y_training_data,pred_train))\n",
    "        print(\"Recall on validation set : \",recall_score(y_validation_data,pred_validation))\n",
    "        print(\"Precision on training set : \",precision_score(y_training_data,pred_train))\n",
    "        print(\"Precision on validation set : \",precision_score(y_validation_data,pred_validation))\n",
    "        print(\"F1-Score on training set : \",f1_score(y_training_data,pred_train))\n",
    "        print(\"F1-Score on validation set : \",f1_score(y_validation_data,pred_validation))\n",
    "    \n",
    "    return score_list # returning the list with train and validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f3ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_of_models(models):\n",
    "    # defining list of models\n",
    "\n",
    "    # defining empty lists to add train and validation results\n",
    "    acc_train = []\n",
    "    acc_validation = []\n",
    "    recall_train = []\n",
    "    recall_validation = []\n",
    "    precision_train = []\n",
    "    precision_validation = []\n",
    "    f1_train =[]\n",
    "    f1_validation = []\n",
    "    roc_auc_train = []\n",
    "    roc_auc_validation = []\n",
    "    \n",
    "    # looping through all the models to get the accuracy, precall and precision scores\n",
    "    for idx,model in enumerate(models):\n",
    "            #print(model)\n",
    "            j = get_metrics_score(model,X_train_df, X_validation, y_train_df, y_validation,False)\n",
    "            acc_train.append(np.round(j[0],2))\n",
    "            acc_validation.append(np.round(j[1],2))\n",
    "            recall_train.append(np.round(j[2],2))\n",
    "            recall_validation.append(np.round(j[3],2))\n",
    "            precision_train.append(np.round(j[4],2))\n",
    "            precision_validation.append(np.round(j[5],2))\n",
    "            f1_train.append(np.round(j[6],2))\n",
    "            f1_validation.append(np.round(j[7],2))\n",
    "    \n",
    "    model_metrics = pd.DataFrame({'TrainAccuracy': acc_train, 'validationAccuracy': acc_validation, 'Train_Recall':recall_train, \n",
    "                                  'validationRecall':recall_validation, 'TrainPrecision':precision_train, 'validationPrecision':precision_validation, \n",
    "                                  'TrainF1Score':f1_train, 'validationF1Score':f1_validation\n",
    "                                 })\n",
    "    \n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a055b28",
   "metadata": {},
   "source": [
    "### Pipeline for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e493017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Importing relevant metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2bbe8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "X_val = df_val.drop(columns = ['Exited'], axis = 1)\n",
    "\n",
    "## Scaling only continuous columns\n",
    "columns_to_scale = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Surname', \n",
    "                    'Balance_per_product', 'Balance_by_est_salary', 'Tenure_age_ratio', 'AgeSurname_mean_churn']\n",
    "\n",
    "## Assigning weigts scale\n",
    "weights_dict = {0 : 1, 1 : 4}\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(criterion = 'entropy', class_weight = weights_dict, max_depth = 4, max_features = None\n",
    "                            , min_samples_split = 25, min_samples_leaf = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5578771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Feature_Encoding&#x27;,\n",
       "                 CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                                    label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                                    onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                                    target_encoding_cols=[&#x27;Surname&#x27;])),\n",
       "                (&#x27;Feature_Extraction&#x27;, AddFeatures()),\n",
       "                (&#x27;Feature_Scaling&#x27;,\n",
       "                 CustomScaler(scaling_cols=[&#x27;CreditScore&#x27;, &#x27;Age&#x27;, &#x27;Tenure&#x27;,\n",
       "                                            &#x27;Balance&#x27;, &#x27;NumOfProducts&#x27;,\n",
       "                                            &#x27;EstimatedSalary&#x27;, &#x27;Surname&#x27;,\n",
       "                                            &#x27;Balance_per_product&#x27;,\n",
       "                                            &#x27;Balance_by_est_salary&#x27;,\n",
       "                                            &#x27;Tenure_age_ratio&#x27;,\n",
       "                                            &#x27;AgeSurname_mean_churn&#x27;])),\n",
       "                (&#x27;classifiers&#x27;,\n",
       "                 DecisionTreeClassifier(class_weight={0: 1, 1: 4},\n",
       "                                        criterion=&#x27;entropy&#x27;, max_depth=4,\n",
       "                                        min_samples_leaf=15,\n",
       "                                        min_samples_split=25))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Feature_Encoding&#x27;,\n",
       "                 CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                                    label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                                    onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                                    target_encoding_cols=[&#x27;Surname&#x27;])),\n",
       "                (&#x27;Feature_Extraction&#x27;, AddFeatures()),\n",
       "                (&#x27;Feature_Scaling&#x27;,\n",
       "                 CustomScaler(scaling_cols=[&#x27;CreditScore&#x27;, &#x27;Age&#x27;, &#x27;Tenure&#x27;,\n",
       "                                            &#x27;Balance&#x27;, &#x27;NumOfProducts&#x27;,\n",
       "                                            &#x27;EstimatedSalary&#x27;, &#x27;Surname&#x27;,\n",
       "                                            &#x27;Balance_per_product&#x27;,\n",
       "                                            &#x27;Balance_by_est_salary&#x27;,\n",
       "                                            &#x27;Tenure_age_ratio&#x27;,\n",
       "                                            &#x27;AgeSurname_mean_churn&#x27;])),\n",
       "                (&#x27;classifiers&#x27;,\n",
       "                 DecisionTreeClassifier(class_weight={0: 1, 1: 4},\n",
       "                                        criterion=&#x27;entropy&#x27;, max_depth=4,\n",
       "                                        min_samples_leaf=15,\n",
       "                                        min_samples_split=25))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CategoricalEncoder</label><div class=\"sk-toggleable__content\"><pre>CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                   label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                   onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                   target_encoding_cols=[&#x27;Surname&#x27;])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AddFeatures</label><div class=\"sk-toggleable__content\"><pre>AddFeatures()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomScaler</label><div class=\"sk-toggleable__content\"><pre>CustomScaler(scaling_cols=[&#x27;CreditScore&#x27;, &#x27;Age&#x27;, &#x27;Tenure&#x27;, &#x27;Balance&#x27;,\n",
       "                           &#x27;NumOfProducts&#x27;, &#x27;EstimatedSalary&#x27;, &#x27;Surname&#x27;,\n",
       "                           &#x27;Balance_per_product&#x27;, &#x27;Balance_by_est_salary&#x27;,\n",
       "                           &#x27;Tenure_age_ratio&#x27;, &#x27;AgeSurname_mean_churn&#x27;])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight={0: 1, 1: 4}, criterion=&#x27;entropy&#x27;,\n",
       "                       max_depth=4, min_samples_leaf=15, min_samples_split=25)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Feature_Encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    label_encoder_cols=['Gender'],\n",
       "                                    onehot_encoder_cols=['Geography'],\n",
       "                                    target_encoding_cols=['Surname'])),\n",
       "                ('Feature_Extraction', AddFeatures()),\n",
       "                ('Feature_Scaling',\n",
       "                 CustomScaler(scaling_cols=['CreditScore', 'Age', 'Tenure',\n",
       "                                            'Balance', 'NumOfProducts',\n",
       "                                            'EstimatedSalary', 'Surname',\n",
       "                                            'Balance_per_product',\n",
       "                                            'Balance_by_est_salary',\n",
       "                                            'Tenure_age_ratio',\n",
       "                                            'AgeSurname_mean_churn'])),\n",
       "                ('classifiers',\n",
       "                 DecisionTreeClassifier(class_weight={0: 1, 1: 4},\n",
       "                                        criterion='entropy', max_depth=4,\n",
       "                                        min_samples_leaf=15,\n",
       "                                        min_samples_split=25))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = Pipeline(steps = [ ('Feature_Encoding', CategoricalEncoder()),\n",
    "                              ('Feature_Extraction', AddFeatures()),\n",
    "                              ('Feature_Scaling', CustomScaler(columns_to_scale)),\n",
    "                              ('classifiers', decision_tree_classifier)\n",
    "                            ]\n",
    "                   )\n",
    "\n",
    "# Fit pipeline with training data\n",
    "dt_model.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "674d7c56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the training data \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.83      5580\n",
      "           1       0.44      0.73      0.55      1460\n",
      "\n",
      "    accuracy                           0.75      7040\n",
      "   macro avg       0.68      0.75      0.69      7040\n",
      "weighted avg       0.82      0.75      0.77      7040\n",
      "\n",
      "Classification report for the validation data \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.85       776\n",
      "           1       0.45      0.74      0.56       184\n",
      "\n",
      "    accuracy                           0.78       960\n",
      "   macro avg       0.69      0.77      0.71       960\n",
      "weighted avg       0.84      0.78      0.80       960\n",
      "\n",
      "Confusion matrix for the train data- \n",
      " [[4242 1338]\n",
      " [ 392 1068]]\n",
      "\n",
      "Confusion matrix for the validation data- \n",
      " [[610 166]\n",
      " [ 47 137]]\n"
     ]
    }
   ],
   "source": [
    "y_dt_train_predicted = dt_model.predict(X)\n",
    "y_dt_validation_predicted = dt_model.predict(X_val)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print('Classification report for the training data \\n',classification_report(y_train, y_dt_train_predicted))\n",
    "print('Classification report for the validation data \\n',classification_report(y_val, y_dt_validation_predicted))\n",
    "print('Confusion matrix for the train data- \\n',confusion_matrix(y_train, y_dt_train_predicted))\n",
    "print('\\nConfusion matrix for the validation data- \\n',confusion_matrix(y_val, y_dt_validation_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab1b54",
   "metadata": {},
   "source": [
    "### Pipeline for RandomForest, LGBM, XGB, Naive Bayes (Gaussian/Multinomial), kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63b6f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data and a few common model parameters\n",
    "X = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "y = y_train.ravel()\n",
    "\n",
    "weights_dict = {0 : 1, 1 : 4}\n",
    "\n",
    "weight = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "494f4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the models to be tried out\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50ed9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing a list of models to try out in the spot-checking process\n",
    "def classification_models(models_dict = dict()):\n",
    "    # Tree models\n",
    "    for n_trees in [21, 1001]:\n",
    "        models_dict['RF_' + str(n_trees)] = RandomForestClassifier(n_estimators = n_trees, n_jobs = -1, criterion = 'entropy'\n",
    "                                                              , class_weight = weights_dict, max_depth = 6, max_features = 0.6\n",
    "                                                              , min_samples_split = 30, min_samples_leaf = 20)\n",
    "        \n",
    "        models_dict['LGBM_' + str(n_trees)] = LGBMClassifier(boosting_type='dart', num_leaves=31, max_depth= 6, \n",
    "                                                             learning_rate=0.1, n_estimators=n_trees, class_weight=weights_dict, \n",
    "                                                             min_child_samples=20, colsample_bytree=0.6, reg_alpha=0.3, \n",
    "                                                             reg_lambda=1.0, n_jobs=- 1, importance_type = 'gain')\n",
    "        \n",
    "        models_dict['XGB_' + str(n_trees)] = XGBClassifier(objective='binary:logistic', n_estimators = n_trees, max_depth = 6\n",
    "                                                      , learning_rate = 0.03, n_jobs = -1, colsample_bytree = 0.6\n",
    "                                                      , reg_alpha = 0.3, reg_lambda = 0.1, scale_pos_weight = weight)\n",
    "        \n",
    "        models_dict['ETC_' + str(n_trees)] = ExtraTreesClassifier(n_estimators=n_trees, criterion = 'entropy', max_depth = 6\n",
    "                                                            , max_features = 0.6, n_jobs = -1, class_weight = weights_dict\n",
    "                                                            , min_samples_split = 30, min_samples_leaf = 20)\n",
    "    \n",
    "    # kNN models\n",
    "    for n in [3,5,11]:\n",
    "        models_dict['KNN_' + str(n)] = KNeighborsClassifier(n_neighbors=n)\n",
    "    \n",
    "    # Naive-Bayes models\n",
    "    models_dict['gauss_nb'] = GaussianNB()\n",
    "    models_dict['multi_nb'] = MultinomialNB()\n",
    "    models_dict['compl_nb'] = ComplementNB()\n",
    "    models_dict['bern_nb'] = BernoulliNB()\n",
    "    \n",
    "    return models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d46380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automation of data preparation and model run through pipelines\n",
    "def make_pipeline(model):\n",
    "    '''\n",
    "    Creates pipeline for the model passed as the argument. Uses standard scaling only in case of kNN models. \n",
    "    Ignores scaling step for tree/Naive Bayes models\n",
    "    '''\n",
    "    \n",
    "    if (str(model).find('KNeighborsClassifier') != -1):\n",
    "        pipe =  Pipeline(steps = [('Feature_Encoding', CategoricalEncoder()),\n",
    "                                  ('Feature_Extraction', AddFeatures()),\n",
    "                                  ('Feature_Scaling', CustomScaler(columns_to_scale)),\n",
    "                                  ('classifiers', model)\n",
    "                             ])\n",
    "    else :\n",
    "        pipe =  Pipeline(steps = [('Feature_Encoding', CategoricalEncoder()),\n",
    "                                  ('Feature_Extraction', AddFeatures()),\n",
    "                                  ('classifiers', model)\n",
    "                                ])\n",
    "    \n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ca10065",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run/Evaluate all 15 models using KFold cross-validation (5 folds)\n",
    "def evaluate_classification_models(X, y, models, folds = 5, metric = 'recall'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # Evaluate model through automated pipelines\n",
    "        pipeline = make_pipeline(model)\n",
    "        scores = cross_val_score(pipeline, X, y, cv = folds, scoring = metric, n_jobs = -1)\n",
    "        \n",
    "        # Store results of the evaluated model\n",
    "        results[name] = scores\n",
    "        mu, sigma = np.mean(scores), np.std(scores)\n",
    "        # Printing individual model results\n",
    "        print('Model {}: mean = {}, std_dev = {}'.format(name, mu, sigma))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77866cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric\n",
      "Model RF_21: mean = 0.7486301369863015, std_dev = 0.04020728800394294\n",
      "Model LGBM_21: mean = 0.7602739726027397, std_dev = 0.02748274335306698\n",
      "Model XGB_21: mean = 0.7410958904109589, std_dev = 0.020387501460461945\n",
      "Model ETC_21: mean = 0.7506849315068493, std_dev = 0.03501869323606633\n",
      "Model RF_1001: mean = 0.7397260273972602, std_dev = 0.0314621481585092\n",
      "Model LGBM_1001: mean = 0.6020547945205479, std_dev = 0.02013279240643707\n",
      "Model XGB_1001: mean = 0.6068493150684932, std_dev = 0.018556461896087777\n",
      "Model ETC_1001: mean = 0.7554794520547945, std_dev = 0.02942815213553539\n",
      "Model KNN_3: mean = 0.4232876712328767, std_dev = 0.011583242825539568\n",
      "Model KNN_5: mean = 0.4013698630136986, std_dev = 0.013595502220054258\n",
      "Model KNN_11: mean = 0.347945205479452, std_dev = 0.017408582228957303\n",
      "Model gauss_nb: mean = 0.04452054794520548, std_dev = 0.060375070879558936\n",
      "Model multi_nb: mean = 0.5410958904109588, std_dev = 0.022613115094820772\n",
      "Model compl_nb: mean = 0.5410958904109588, std_dev = 0.022613115094820772\n",
      "Model bern_nb: mean = 0.32945205479452055, std_dev = 0.021376008951133195\n",
      "F1-score metric\n",
      "Model RF_21: mean = 0.5913358823539209, std_dev = 0.010804334667124509\n",
      "Model LGBM_21: mean = 0.5906710853891826, std_dev = 0.011849462342557224\n",
      "Model XGB_21: mean = 0.5845926914794305, std_dev = 0.011813054710743443\n",
      "Model ETC_21: mean = 0.5834125923933254, std_dev = 0.01588623508357455\n",
      "Model RF_1001: mean = 0.5932753728969921, std_dev = 0.005649183872844379\n",
      "Model LGBM_1001: mean = 0.5946246878390624, std_dev = 0.009326719904954582\n",
      "Model XGB_1001: mean = 0.6162345951184942, std_dev = 0.008704366741816188\n",
      "Model ETC_1001: mean = 0.5892876306405899, std_dev = 0.013263045681179723\n",
      "Model KNN_3: mean = 0.49620064845448714, std_dev = 0.01201496791105073\n",
      "Model KNN_5: mean = 0.4960913655775996, std_dev = 0.021539473208299777\n",
      "Model KNN_11: mean = 0.467989821356989, std_dev = 0.017285536105973852\n",
      "Model gauss_nb: mean = 0.07001518642647675, std_dev = 0.08722433653443283\n",
      "Model multi_nb: mean = 0.33602550067449893, std_dev = 0.012851226290521623\n",
      "Model compl_nb: mean = 0.33595435259698886, std_dev = 0.012872834383415802\n",
      "Model bern_nb: mean = 0.3548169225813379, std_dev = 0.02107080895763649\n"
     ]
    }
   ],
   "source": [
    "# Spot-checking in action\n",
    "models = classification_models()\n",
    "print('Recall metric')\n",
    "results = evaluate_classification_models(X, y , models, metric = 'recall')\n",
    "print('F1-score metric')\n",
    "results = evaluate_classification_models(X, y , models, metric = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ab900",
   "metadata": {},
   "source": [
    "### Let us try LGBM Model with HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7d89304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model_01 = LGBMClassifier(boosting_type='dart', num_leaves=45, max_depth= 6, \n",
    "                               learning_rate=0.1, n_estimators=90, class_weight={0 : 1, 1 : 3}, \n",
    "                               min_child_samples=20, colsample_bytree=0.6, reg_alpha=0.3, \n",
    "                               reg_lambda=1.0, n_jobs=- 1, importance_type = 'gain', force_col_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3e563383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_eval\n",
      "[LightGBM] [Info] Number of positive: 1460, number of negative: 5580\n",
      "[LightGBM] [Info] Total Bins 1921\n",
      "[LightGBM] [Info] Number of data points in the train set: 7040, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.439759 -> initscore=-0.242140\n",
      "[LightGBM] [Info] Start training from score -0.242140\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Feature_Encoding&#x27;,\n",
       "                 CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                                    label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                                    onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                                    target_encoding_cols=[&#x27;Surname&#x27;])),\n",
       "                (&#x27;Feature_Extraction&#x27;, AddFeatures()),\n",
       "                (&#x27;classifiers&#x27;,\n",
       "                 LGBMClassifier(boosting_type=&#x27;dart&#x27;, class_weight={0: 1, 1: 3},\n",
       "                                colsample_bytree=0.6, force_col_wise=True,\n",
       "                                importance_type=&#x27;gain&#x27;, max_depth=6,\n",
       "                                n_estimators=90, n_jobs=-1, num_leaves=45,\n",
       "                                reg_alpha=0.3, reg_lambda=1.0,\n",
       "                                verbose_eval=False))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\" ><label for=\"sk-estimator-id-93\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Feature_Encoding&#x27;,\n",
       "                 CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                                    label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                                    onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                                    target_encoding_cols=[&#x27;Surname&#x27;])),\n",
       "                (&#x27;Feature_Extraction&#x27;, AddFeatures()),\n",
       "                (&#x27;classifiers&#x27;,\n",
       "                 LGBMClassifier(boosting_type=&#x27;dart&#x27;, class_weight={0: 1, 1: 3},\n",
       "                                colsample_bytree=0.6, force_col_wise=True,\n",
       "                                importance_type=&#x27;gain&#x27;, max_depth=6,\n",
       "                                n_estimators=90, n_jobs=-1, num_leaves=45,\n",
       "                                reg_alpha=0.3, reg_lambda=1.0,\n",
       "                                verbose_eval=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CategoricalEncoder</label><div class=\"sk-toggleable__content\"><pre>CategoricalEncoder(cols=[&#x27;Surname&#x27;, &#x27;Geography&#x27;, &#x27;Gender&#x27;],\n",
       "                   label_encoder_cols=[&#x27;Gender&#x27;],\n",
       "                   onehot_encoder_cols=[&#x27;Geography&#x27;],\n",
       "                   target_encoding_cols=[&#x27;Surname&#x27;])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AddFeatures</label><div class=\"sk-toggleable__content\"><pre>AddFeatures()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\" ><label for=\"sk-estimator-id-96\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(boosting_type=&#x27;dart&#x27;, class_weight={0: 1, 1: 3},\n",
       "               colsample_bytree=0.6, force_col_wise=True,\n",
       "               importance_type=&#x27;gain&#x27;, max_depth=6, n_estimators=90, n_jobs=-1,\n",
       "               num_leaves=45, reg_alpha=0.3, reg_lambda=1.0,\n",
       "               verbose_eval=False)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Feature_Encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    label_encoder_cols=['Gender'],\n",
       "                                    onehot_encoder_cols=['Geography'],\n",
       "                                    target_encoding_cols=['Surname'])),\n",
       "                ('Feature_Extraction', AddFeatures()),\n",
       "                ('classifiers',\n",
       "                 LGBMClassifier(boosting_type='dart', class_weight={0: 1, 1: 3},\n",
       "                                colsample_bytree=0.6, force_col_wise=True,\n",
       "                                importance_type='gain', max_depth=6,\n",
       "                                n_estimators=90, n_jobs=-1, num_leaves=45,\n",
       "                                reg_alpha=0.3, reg_lambda=1.0,\n",
       "                                verbose_eval=False))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model = Pipeline(steps = [ ('Feature_Encoding', CategoricalEncoder()),\n",
    "                              ('Feature_Extraction', AddFeatures()),\n",
    "                              # ('Feature_Scaling', CustomScaler(columns_to_scale)),\n",
    "                              ('classifiers', lgbm_model_01)\n",
    "                            ]\n",
    "                   )\n",
    "\n",
    "# Fit pipeline with training data\n",
    "lgbm_model.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d1df1f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for the training data \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      5580\n",
      "           1       0.65      0.74      0.69      1460\n",
      "\n",
      "    accuracy                           0.86      7040\n",
      "   macro avg       0.79      0.82      0.80      7040\n",
      "weighted avg       0.87      0.86      0.87      7040\n",
      "\n",
      "Classification report for the validation data \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       776\n",
      "           1       0.54      0.70      0.61       184\n",
      "\n",
      "    accuracy                           0.83       960\n",
      "   macro avg       0.73      0.78      0.75       960\n",
      "weighted avg       0.85      0.83      0.84       960\n",
      "\n",
      "Confusion matrix for the train data- \n",
      " [[4994  586]\n",
      " [ 380 1080]]\n",
      "\n",
      "Confusion matrix for the validation data- \n",
      " [[667 109]\n",
      " [ 56 128]]\n"
     ]
    }
   ],
   "source": [
    "y_dt_train_predicted = lgbm_model.predict(X)\n",
    "y_dt_validation_predicted = lgbm_model.predict(X_val)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print('Classification report for the training data \\n',classification_report(y_train, y_dt_train_predicted))\n",
    "print('Classification report for the validation data \\n',classification_report(y_val, y_dt_validation_predicted))\n",
    "print('Confusion matrix for the train data- \\n',confusion_matrix(y_train, y_dt_train_predicted))\n",
    "print('\\nConfusion matrix for the validation data- \\n',confusion_matrix(y_val, y_dt_validation_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
